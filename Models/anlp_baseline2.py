# -*- coding: utf-8 -*-
"""ANLP_Baseline2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wsYzoJnhmY5oyWZklQIx-Y1z-1PxdQJ2

# Dataset
"""

!pip install -U sentence-transformers
!pip install datasets
!pip install transformers==4.21.3 
!pip install bert-extractive-summarizer

import torch
from datasets import load_dataset
answersumm = load_dataset("alexfabbri/answersumm")

from summarizer import Summarizer
body = """
The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.
"""
extractive_summary_model = Summarizer()
result = extractive_summary_model(body, ratio=0.4)  # Specified with ratio

#result

from sentence_transformers import SentenceTransformer
sentences = ["This is an example sentence", "Each sentence is converted"]

embedding_model = SentenceTransformer('sentence-transformers/paraphrase-albert-small-v2')
embeddings = embedding_model.encode(sentences)
print(embeddings)

from numpy import dot
from numpy.linalg import norm

def cosine_similarity(embed1,embed2):
  cos_sim = dot(embed1, embed2)/(norm(embed1)*norm(embed2))
  return cos_sim

from sklearn import preprocessing
from sklearn.cluster import KMeans
import numpy as np

from transformers import pipeline
#summarizer = pipeline("summarization")

#summary_text = summarizer(cluster_text[0])

#print(summary_text[0]["summary_text"])

summarizer = pipeline("summarization", model="t5-base", tokenizer="t5-base")

"""## MODEL"""

MAX_COUNT = 10

# Extract the questions
questions = []
for i in range(MAX_COUNT):
  all_full_answers = []
  for answer in answersumm["train"][i]["answers"]:
    full_answer = []

    for sentence in answer['sents']:
      full_answer.append(sentence["text"])

    fullanswer = " ".join(full_answer)
    all_full_answers.append(fullanswer)

  passage = ""
  for summary in all_full_answers:
    passage = passage + summary + " "

  questions.append(passage[:-1])

# Extract the corresponding summaries
summaries = []
for i in range(MAX_COUNT):
  for summ in answersumm["train"]["summaries"][i]:
    full_summary = []

    for sentence in summ:
      full_summary.append(sentence)

    fullsummary = " ".join(full_summary)
  summaries.append(fullsummary)

# Run the model
model_text = []
for i in range(MAX_COUNT):
  all_answers = []
  all_full_answers = []
  for answer in answersumm["train"][i]["answers"]:
      full_answer = []
      for sentence in answer['sents']:
        full_answer.append(sentence["text"])
      fullanswer = " ".join(full_answer)
      all_full_answers.append(fullanswer)
      all_answers.append(extractive_summary_model(fullanswer,ratio=0.6))
  
  all_sentences = []
  for summary in all_answers:
    all_sentences = all_sentences + summary.split(". ")


  for ind,summary in enumerate(all_sentences):
    if all_sentences[ind][-1] == ".":
      all_sentences[ind] = all_sentences[ind][:-1]


  embeddings = embedding_model.encode(all_sentences)
  length = np.sqrt((embeddings**2).sum(axis=1))[:,None]
  embeddings = embeddings / length

  kmeans = KMeans(n_clusters=5, random_state=0).fit(embeddings)
  len_ = np.sqrt(np.square(kmeans.cluster_centers_).sum(axis=1)[:,None])
  centers = kmeans.cluster_centers_ / len_
  dist = 1 - np.dot(centers, embeddings.T) 




  cluster_ids = []
  for j in range(len(embeddings)):
    cluster_ids.append(0)

  for vecid,vec in enumerate(embeddings):
    mindist = 100000000
    for id,center in enumerate(centers):
      distance = np.sum( (np.array(vec) - np.array(center))** 2)
      if distance < mindist:
        mindist = distance
        cluster_ids[vecid] = id

  cluster_text = []
  for j in range(5):
    cluster_text.append("")

  for sent_id,id in enumerate(cluster_ids):
    cluster_text[id] = cluster_text[id] + all_sentences[sent_id] + ". "

  finalsummary = ""
  for cluster in cluster_text:
    summary_text = summarizer(cluster, min_length=5,max_length=100)[0]['summary_text']
    finalsummary = finalsummary + summary_text
  print(i)
  print(finalsummary,summaries[i])
  summary_text = summarizer(finalsummary, truncation=True, max_length=50, min_length=5, do_sample=False)[0]['summary_text']
  model_text.append(summary_text)

!pip install rouge

from rouge import Rouge
rouge = Rouge()

# Calculate Rouge Scores
avg_r1 = 0
avg_r2 = 0
avg_rl = 0
for i in range(MAX_COUNT):
  scores = rouge.get_scores(model_text[i], summaries[i])
  avg_r1 += scores[0]["rouge-1"]['f']
  avg_r2 += scores[0]["rouge-2"]['f']
  avg_rl += scores[0]["rouge-l"]['f']
           

avg_r1 = avg_r1 * 100.0 / MAX_COUNT
avg_r2 = avg_r2 * 100.0 / MAX_COUNT
avg_rl = avg_rl * 100.0 / MAX_COUNT
avg_r1,avg_r2,avg_rl