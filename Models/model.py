# -*- coding: utf-8 -*-
"""Model2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g8jdLk9rXmPPqYrDW6rn9for_N7u2dFo
"""

!pip install -U sentence-transformers
!pip install datasets
!pip install transformers==4.21.3 
!pip install bert-extractive-summarizer



import torch
from datasets import load_dataset
answersumm = load_dataset("alexfabbri/answersumm")

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

from summarizer import Summarizer
body = """
The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.
"""
extractive_summary_model = Summarizer()

"""# PreProcessing"""

MAX_COUNT = 3000

# Extract the questions
questions = []
for i in range(0,MAX_COUNT):
  question = answersumm["test"][i]["question"]["title"].lower()
  questions.append(question)

labels = []
answers = []
for i in range(0,MAX_COUNT):
  answer_list = []
  label_list = []
  for answer in answersumm["test"][i]["answers"]:
    answer_sentences = []
    label_sentences = []
    for sentence in answer['sents']:
      answer_sentences.append(sentence["text"].lower())
      label_sentences.append(sentence["label"][0])
    answer_list.append(answer_sentences)
    label_list.append(label_sentences)
  answers.append(answer_list)
  labels.append(label_list)
len(answers[0])

input = []
output_labels = []
all_summaries = []
for ind,answer_val in enumerate(answers):
  final_summary = " "
  max_score = -1
  for val,answer in enumerate(answer_val):
    final_sentence = "<sep> "
    for sentence in answer:
      final_sentence = final_sentence +  sentence + " <sep> "
    score = int(answersumm["test"][ind]["answers"][val]["answer_details"]["score"])
    
    max_score = max(score,max_score)

    final_summary = final_summary + extractive_summary_model(final_sentence, ratio=0.6) + " "
  all_summaries.append(final_summary)

all_summaries[0]

for summary in all_summaries:
  sentence_list = summary.split("<sep>")
  sentence_list = sentence_list[1:]
  print(sentence_list)

from sentence_transformers import SentenceTransformer
sentences = ["This is an example sentence", "Each sentence is converted"]

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
embeddings = model.encode(sentences)
print(embeddings)

from sentence_transformers import SentenceTransformer, util

"""
query = "How many people live in London?"
docs = ["Around 9 Million people live in London", "London is known for its financial district"]

#Load the model
model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')

#Encode query and documents
query_emb = model.encode(query)
doc_emb = model.encode(docs)

#Compute dot score between query and all document embeddings
scores = util.dot_score(query_emb, doc_emb)[0].tolist()

#Output passages & scores
for ind,score in enumerate(scores):
    print(score, docs[ind])
"""

summary_encodings = []
for summary in all_summaries:
  sentence_list = summary.split("<sep>")
  sentence_list = sentence_list[1:]

  sent_encodings = []
  for sent in sentence_list:
    sent_encodings.append(model.encode(sent))
  summary_encodings.append(sent_encodings)

len(summary_encodings[9])

import numpy as np

"""
doc_emb = model.encode(docs)
scores = util.dot_score(doc_emb[0]/np.linalg.norm(doc_emb[0]), doc_emb[1]/np.linalg.norm(doc_emb[1]))[0]
scores
"""

from sklearn.cluster import AgglomerativeClustering


X = np.array([[1, 2], [1, 4], [1, 0],
              [4, 2], [4, 4], [4, 0]])
clustering = AgglomerativeClustering(n_clusters=None,distance_threshold=0.65,affinity='cosine',linkage="complete").fit(X)
clustering

clustering.labels_

from collections import Counter
cluster_summaries = []
for index,sent_encodings in enumerate(summary_encodings):
  clustering = AgglomerativeClustering(n_clusters=None,distance_threshold=0.65,affinity='cosine',linkage="complete").fit(sent_encodings)
  #print(clustering.labels_)
  clusters = clustering.labels_
  counts = Counter(clusters)
  intital_counts = Counter(clusters)
  num = 1
  while counts[num] != 0:
    counts[num] = counts[num] + counts[num - 1]
    num += 1
  summary_len = int(1 * counts[num - 1])
  cluster_count = summary_len * 0.5 / num
  #print(cluster_count)
  
  
  cluster_summary = ""
  #print(num)
  taken = Counter()

  for i in range(0,num):
    if intital_counts[clusters[i]] > cluster_count:
      taken[i] = 1

  final_sum = ""
  sentence_list = all_summaries[index].split("<sep>")
  sentence_list = sentence_list[1:]
  for i,sentence in enumerate(sentence_list):
    if taken[i] == 1:
      final_sum = final_sum + sentence + " "
  cluster_summaries.append(final_sum)

cluster_summaries[0]

"""
from transformers import pipeline
summarizer = pipeline("summarization", model="t5-base", tokenizer="t5-base")
"""

from transformers import BartForConditionalGeneration, AutoTokenizer

from transformers import TrainingArguments, Trainer
from transformers import DataCollatorForSeq2Seq

model = torch.load("pytorch_model.bin")
model_ckpt = "pytorch_model.bin"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
summarizer = BartForConditionalGeneration.from_pretrained(model_ckpt)
seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=summarizer)

training_args = TrainingArguments(output_dir='bart-multi-news', num_train_epochs=1, warmup_steps=500, per_device_train_batch_size=1, per_device_eval_batch_size=1, weight_decay=0.01, logging_steps=10, push_to_hub=False, 
evaluation_strategy='steps', eval_steps=500, save_steps=1e6, 
gradient_accumulation_steps=16)

trainer = Trainer(model=summarizer, args=training_args, tokenizer=tokenizer, data_collator=seq2seq_data_collator, train_dataset=answersumm["train"], eval_dataset=answersumm["test"])
trainer.train()

final_summaries = []
i = 0
for summary in cluster_summaries:
  final_summaries.append(summarizer(summary, truncation=True, max_length=150, min_length=50, do_sample=False)[0]['summary_text'])
  i = i +1
  print(i)

final_summaries[0]

!pip install rouge

from rouge import Rouge
rouge = Rouge()

# Calculate Rouge Scores
avg_r1 = 0
avg_r2 = 0
avg_rl = 0
for i in range(MAX_COUNT):
  scores = rouge.get_scores(final_summaries[i],answersumm["test"]["summaries"][i][0][0] + " " +  answersumm["test"]["summaries"][i][0][1] )
  avg_r1 += scores[0]["rouge-1"]['f']
  avg_r2 += scores[0]["rouge-2"]['f']
  avg_rl += scores[0]["rouge-l"]['f']
           

avg_r1 = avg_r1 * 100.0 / MAX_COUNT
avg_r2 = avg_r2 * 100.0 / MAX_COUNT
avg_rl = avg_rl * 100.0 / MAX_COUNT
avg_r1,avg_r2,avg_rl

answersumm["train"]["summaries"][0][0][0] + " " +  answersumm["train"]["summaries"][0][0][1]

for i in range(MAX_COUNT):
    print("Question - " + answersumm["test"]["question"][i]["question"])
    print("Precited Summary - " + final_summaries[i])
    print("Final Summary - " + answersumm["test"]["summaries"][i][0][0] + " " +  answersumm["train"]["summaries"][i][0][1])
    print("-------")
    print()

answersumm["train"]["answers"][0][1]["answer_details"]["score"]

answersumm["train"][0]["answers"]

answers[0][0]

22.008672950106476, 3.890870395542337, 19.584650420435885

